{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data needed for the code\n",
    "# dob_job_application_filings_subset.csv,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Read the file into a DataFrame: df\n",
    "df = pd.read_csv('dob_job_application_filings_subset.csv')\n",
    "\n",
    "# Print the head of df\n",
    "print(df.head())\n",
    "\n",
    "# Print the tail of df\n",
    "print(df.tail())\n",
    "\n",
    "# Print the shape of df\n",
    "print(df.shape)\n",
    "\n",
    "# Print the columns of df\n",
    "print(df.columns)\n",
    "\n",
    "# Print the head and tail of df_subset\n",
    "print(df_subset.head())\n",
    "print(df_subset.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print the info of df\n",
    "print(df.info())\n",
    "\n",
    "# Print the info of df_subset\n",
    "print(df_subset.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# As you've seen, .describe() can only be used on numeric columns.\n",
    "# So how can you diagnose data issues when you have categorical data?\n",
    "# One way is by using the .value_counts() method, which returns the\n",
    "# frequency counts for each unique value in a column!\n",
    "\n",
    "# This method also has an optional parameter called dropna which is\n",
    "# True by default. What this means is if you have missing data in a\n",
    "# column, it will not give a frequency count of them. You want to set\n",
    "# the dropna column to False so if there are missing values in a column,\n",
    "# it will give you the frequency counts.\n",
    "\n",
    "# Print the value counts for 'Borough'\n",
    "print(df['Borough'].value_counts(dropna=False))\n",
    "\n",
    "# Print the value_counts for 'State'\n",
    "print(df.State.value_counts(dropna = False))\n",
    "\n",
    "# Print the value counts for 'Site Fill'\n",
    "print(df['Site Fill'].value_counts(dropna = False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The .plot() method allows you to create a plot of each column of a\n",
    "# DataFrame. The kind parameter allows you to specify the type of plot\n",
    "# to use - kind='hist', for example, plots a histogram.\n",
    "\n",
    "# You'll notice that there are extremely large differences between the\n",
    "# min and max values, and the plot will need to be adjusted accordingly.\n",
    "# In such cases, it's good to look at the plot on a log scale. The\n",
    "# keyword arguments logx=True or logy=True can be passed in to .plot()\n",
    "# depending on which axis you want to rescale.\n",
    "\n",
    "# Create a histogram of the 'Existing Zoning Sqft' column. Rotate the axis\n",
    "# labels by 70 degrees and use a log scale for both axes.\n",
    "\n",
    "# Import matplotlib.pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the histogram\n",
    "df['Existing Zoning Sqft'].plot(kind='hist', rot=70, logx=True, logy=True)\n",
    "\n",
    "# Display the histogram\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the boxplot\n",
    "df.boxplot(column='initial_cost', by='Borough', rot=90)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Boxplots are great when you have a numeric column that you want to\n",
    "# compare across different categories. When you want to visualize two\n",
    "# numeric columns, scatter plots are ideal.\n",
    "\n",
    "# Import necessary modules\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create and display the first scatter plot\n",
    "df.plot(kind='scatter', x='initial_cost', y='total_est_fee', rot=70)\n",
    "plt.show()\n",
    "\n",
    "# Create and display the second scatter plot\n",
    "df_subset.plot(kind='scatter', x='initial_cost', y='total_est_fee', rot=70)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For data to be tidy, it must have:\n",
    "#   Each variable as a separate column.\n",
    "#   Each row as a separate observation.\n",
    "# As a data scientist, you'll encounter data that is represented in a\n",
    "# variety of different ways, so it is important to be able to recognize tidy (or untidy) data when you see it.\n",
    "\n",
    "# pd.melt(). There are two parameters you should be aware of: id_vars and value_vars.\n",
    "# The id_vars represent the columns of the data you do not want to melt (i.e.,\n",
    "# keep it in its current shape), while the value_vars represent the columns you do\n",
    "# wish to melt into rows. By default, if no value_vars are provided, all columns\n",
    "# not set in the id_vars will be melted.\n",
    "\n",
    "# Print the head of airquality\n",
    "print(airquality.head())\n",
    "\n",
    "# Melt airquality: airquality_melt\n",
    "airquality_melt = pd.melt(airquality, id_vars=['Month','Day'])\n",
    "\n",
    "# Print the head of airquality_melt\n",
    "print(airquality_melt.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# When melting DataFrames, it would be better to have column names more \n",
    "# meaningful than variable and value.\n",
    "\n",
    "# The default names may work in certain situations, but it's best to \n",
    "# always have data that is self explanatory.\n",
    "\n",
    "# You can rename the variable column by specifying an argument to the\n",
    "# var_name parameter, and the value column by specifying an argument to\n",
    "# the value_name parameter. You will now practice doing exactly this. The\n",
    "# DataFrame airquality has been pre-loaded for you.\n",
    "\n",
    "# Print the head of airquality\n",
    "print(airquality.head())\n",
    "\n",
    "# Melt airquality: airquality_melt\n",
    "airquality_melt = pd.melt(airquality, id_vars=['Month', 'Day'], var_name='measurement', value_name='reading')\n",
    "\n",
    "# Print the head of airquality_melt\n",
    "print(airquality_melt.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# While melting takes a set of columns and turns it into a single column,\n",
    "# pivoting will create a new column for each unique value in a specified column.\n",
    "\n",
    "# .pivot_table() has an index parameter which you can use to specify the columns\n",
    "# that you don't want pivoted: It is similar to the id_vars parameter of pd.melt().\n",
    "# Two other parameters that you have to specify are columns (the name of the column\n",
    "# you want to pivot), and values (the values to be used when the column is pivoted).\n",
    "# Print the head of airquality_melt\n",
    "print(airquality_melt.head())\n",
    "\n",
    "# Pivot airquality_melt: airquality_pivot\n",
    "airquality_pivot = airquality_melt.pivot_table(index=['Month', 'Day'], columns='measurement', values='reading')\n",
    "\n",
    "# Print the head of airquality_pivot\n",
    "print(airquality_pivot.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# There's a very simple method you can use to get back the original \n",
    "# DataFrame from the pivoted DataFrame: .reset_index(). \n",
    "\n",
    "# Print the index of airquality_pivot\n",
    "print(airquality_pivot.index)\n",
    "\n",
    "# Reset the index of airquality_pivot: airquality_pivot\n",
    "airquality_pivot = airquality_pivot.reset_index()\n",
    "\n",
    "# Print the new index of airquality_pivot\n",
    "print(airquality_pivot.index)\n",
    "\n",
    "# Print the head of airquality_pivot\n",
    "print(airquality_pivot.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pivot airquality_dup: airquality_pivot\n",
    "airquality_pivot = airquality_dup.pivot_table(index=['Month', 'Day'], columns='measurement', values='reading', aggfunc=np.mean)\n",
    "\n",
    "# Reset the index of airquality_pivot\n",
    "airquality_pivot = airquality_pivot.reset_index()\n",
    "\n",
    "# Print the head of airquality_pivot\n",
    "print(airquality_pivot.head())\n",
    "\n",
    "# Print the head of airquality\n",
    "print(airquality.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Melt tb: tb_melt\n",
    "tb_melt = pd.melt(frame = tb, id_vars=['country', 'year'])\n",
    "\n",
    "# Create the 'gender' column\n",
    "tb_melt['gender'] = tb_melt.variable.str[0]\n",
    "\n",
    "# Create the 'age_group' column\n",
    "tb_melt['age_group'] = tb_melt.variable.str[1:]\n",
    "\n",
    "# Print the head of tb_melt\n",
    "print(tb_melt.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Melt ebola: ebola_melt\n",
    "ebola_melt = pd.melt(ebola, id_vars=['Date', 'Day'], var_name='type_country', value_name='counts')\n",
    "\n",
    "# Create the 'str_split' column\n",
    "ebola_melt['str_split'] = ebola_melt.type_country.str.split('_')\n",
    "\n",
    "# Create the 'type' column\n",
    "ebola_melt['type'] = ebola_melt.str_split.str.get(0)\n",
    "\n",
    "# Create the 'country' column\n",
    "ebola_melt['country'] = ebola_melt.str_split.str.get(1)\n",
    "\n",
    "# Print the head of ebola_melt\n",
    "print(ebola_melt.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concatenate uber1, uber2, and uber3: row_concat\n",
    "row_concat = pd.concat([uber1,uber2,uber3])\n",
    "\n",
    "# Print the shape of row_concat\n",
    "print(row_concat.shape)\n",
    "\n",
    "# Print the head of row_concat\n",
    "print(row_concat.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Think of column-wise concatenation of data as stitching data\n",
    "# together from the sides instead of the top and bottom. To perform\n",
    "# this action, you use the same pd.concat() function, but this time\n",
    "# with the keyword argument axis=1. The default, axis=0, is for\n",
    "# a row-wise concatenation.\n",
    "\n",
    "# Concatenate ebola_melt and status_country column-wise: ebola_tidy\n",
    "ebola_tidy = pd.concat([ebola_melt,status_country],axis = 1)\n",
    "\n",
    "# Print the shape of ebola_tidy\n",
    "print(ebola_tidy.shape)\n",
    "\n",
    "# Print the head of ebola_tidy\n",
    "print(ebola_tidy.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# glob module has a function called glob that takes a pattern and\n",
    "# returns a list of the files in the working directory that match that pattern.\n",
    "\n",
    "# Import necessary modules\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Write the pattern: pattern\n",
    "pattern = '*.csv'\n",
    "\n",
    "# Save all file matches: csv_files\n",
    "csv_files = glob.glob(pattern)\n",
    "\n",
    "# Print the file names\n",
    "print(csv_files)\n",
    "\n",
    "# Load the second file into a DataFrame: csv2\n",
    "csv2 = pd.read_csv(csv_files[1])\n",
    "\n",
    "# Print the head of csv2\n",
    "print(csv2.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an empty list: frames\n",
    "frames = []\n",
    "\n",
    "#  Iterate over csv_files\n",
    "for csv in csv_files:\n",
    "\n",
    "    #  Read csv into a DataFrame: df\n",
    "    df = pd.read_csv(csv)\n",
    "    \n",
    "    # Append df to frames\n",
    "    frames.append(df)\n",
    "\n",
    "# Concatenate frames into a single DataFrame: uber\n",
    "uber = pd.concat(frames)\n",
    "\n",
    "# Print the shape of uber\n",
    "print(uber.shape)\n",
    "\n",
    "# Print the head of uber\n",
    "print(uber.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merging data allows you to combine disparate datasets into a \n",
    "# single dataset to do more complex analysis.\n",
    "\n",
    "# Merge the DataFrames: o2o\n",
    "o2o = pd.merge(left=site, right=visited, left_on='name', right_on='site')\n",
    "\n",
    "# Print o2o\n",
    "print(o2o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In a many-to-one (or one-to-many) merge, one of the values will be\n",
    "# duplicated and recycled in the output. That is, one of the keys in\n",
    "# the merge is not unique.\n",
    "# Merge the DataFrames: m2o\n",
    "m2o = pd.merge(left=site, right=visited, left_on='name', right_on='site')\n",
    "\n",
    "\n",
    "# Print m2o\n",
    "print(m2o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The final merging scenario occurs when both DataFrames do not have\n",
    "# unique keys for a merge. What happens here is that for each duplicated\n",
    "# key, every pairwise combination will be created.\n",
    "\n",
    "# Merge site and visited: m2m\n",
    "m2m = pd.merge(left=site, right=visited, left_on='name', right_on='site')\n",
    "\n",
    "# Merge m2m and survey: m2m\n",
    "m2m = pd.merge(left=m2m, right=survey, left_on='ident', right_on='taken')\n",
    "\n",
    "# Print the first 20 lines of m2m\n",
    "print(m2m.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In this exercise, you'll see how ensuring all categorical variables\n",
    "# in a DataFrame are of type category reduces memory usage.\n",
    "# Convert the sex column to type 'category'\n",
    "tips.sex = tips.sex.astype('category')\n",
    "\n",
    "# Convert the smoker column to type 'category'\n",
    "tips.smoker = tips.smoker.astype('category')\n",
    "\n",
    "# Print the info of tips\n",
    "print(tips.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If you expect the data type of a column to be numeric (int or float),\n",
    "# but instead it is of type object, this typically means that there is\n",
    "# a non numeric value in the column, which also signifies bad data.\n",
    "\n",
    "# You can use the pd.to_numeric() function to convert a column into a\n",
    "# numeric data type. If the function raises an error, you can be sure\n",
    "# that there is a bad value within the column. You can either use the\n",
    "# techniques you learned in Chapter 1 to do some exploratory data analysis\n",
    "# and find the bad value, or you can choose to ignore or coerce the\n",
    "# value into a missing value, NaN.\n",
    "\n",
    "# Convert 'total_bill' to a numeric dtype\n",
    "tips['total_bill'] = pd.to_numeric(tips['total_bill'], errors='coerce')\n",
    "\n",
    "# Convert 'tip' to a numeric dtype\n",
    "tips['tip'] = pd.to_numeric(tips['tip'],errors='coerce')\n",
    "\n",
    "# Print the info of tips\n",
    "print(tips.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# The regular expression module in python is re. When performing pattern\n",
    "# matching on data, since the pattern will be used for a match across\n",
    "# multiple rows, it's better to compile the pattern first using re.compile(),\n",
    "# and then use the compiled pattern to match values.\n",
    "\n",
    "# Import the regular expression module\n",
    "import re\n",
    "\n",
    "# Compile the pattern: prog\n",
    "prog = re.compile('\\d{3}-\\d{3}-\\d{4}')\n",
    "\n",
    "# See if the pattern matches\n",
    "result = prog.match('123-456-7890')\n",
    "print(bool(result))\n",
    "\n",
    "# See if the pattern matches\n",
    "result = prog.match('1123-456-7890')\n",
    "print(bool(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10', '1']\n"
     ]
    }
   ],
   "source": [
    "# Extracting numbers from strings is a common task, particularly when\n",
    "# working with unstructured data or log files.\n",
    "# Say you have the following string: 'the recipe calls for 6 strawberries\n",
    "# and 2 bananas'.\n",
    "# It would be useful to extract the 6 and the 2 from this string to be saved\n",
    "# for later use when comparing strawberry to banana ratios.\n",
    "# When using a regular expression to extract multiple numbers (or multiple\n",
    "# pattern matches, to be exact), you can use the re.findall() function. Dan\n",
    "# did not discuss this in the video, but it is straightforward to use: You\n",
    "# pass in a pattern and a string to re.findall(), and it will return\n",
    "# a list of the matches.\n",
    "\n",
    "# Import the regular expression module\n",
    "import re\n",
    "\n",
    "# Find the numeric values: matches\n",
    "matches = re.findall('\\d+', 'the recipes calls for 10 strawberries and 1 banana')\n",
    "\n",
    "# Print the matches\n",
    "print(matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Write the first pattern\n",
    "pattern1 = bool(re.match(pattern='\\d{3}-\\d{3}-\\d{4}', string='123-456-7890'))\n",
    "print(pattern1)\n",
    "\n",
    "# Write the second pattern\n",
    "pattern2 = bool(re.match(pattern='\\$\\d{3}\\.\\d{2}', string='$123.45'))\n",
    "print(pattern2)\n",
    "\n",
    "# Write the third pattern\n",
    "pattern3 = bool(re.match(pattern='[A-Z]\\w*', string='Australia'))\n",
    "print(pattern3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The tips dataset has been pre-loaded into a DataFrame called tips.\n",
    "# It has a 'sex' column that contains the values 'Male' or 'Female'.\n",
    "# Your job is to write a function that will recode 'Male' to 1, 'Female'\n",
    "# to 0, and return np.nan for all entries of 'sex' that are neither 'Male' nor 'Female'.\n",
    "\n",
    "# you can use the .apply() method to apply a function across entire rows or columns\n",
    "# of DataFrames. However, note that each column of a DataFrame is a pandas Series.\n",
    "# Functions can also be applied across Series. Here, you will apply your function over the 'sex' column.\n",
    "\n",
    "# Define recode_sex()\n",
    "def recode_sex(sex_value):\n",
    "\n",
    "    # Return 1 if sex_value is 'Male'\n",
    "    if sex_value == 'Male':\n",
    "        return 1\n",
    "    \n",
    "    # Return 0 if sex_value is 'Female'    \n",
    "    elif sex_value == 'Female':\n",
    "        return 0\n",
    "    \n",
    "    # Return np.nan    \n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "# Apply the function to the sex column\n",
    "tips['sex_recode'] = tips.sex.apply(recode_sex)\n",
    "\n",
    "# Print the first five rows of tips\n",
    "print(tips.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You'll now be introduced to a powerful Python feature that will help\n",
    "# you clean your data more effectively: lambda functions. Instead of using\n",
    "# the def syntax that you used in the previous exercise, lambda functions\n",
    "# let you make simple, one-line functions.\n",
    "\n",
    "# For example, here's a function that squares a variable used in an .apply() method:\n",
    "\n",
    "# def my_square(x):\n",
    "#     return x ** 2\n",
    "\n",
    "# df.apply(my_square)\n",
    "# The equivalent code using a lambda function is:\n",
    "\n",
    "# df.apply(lambda x: x ** 2)\n",
    "# The lambda function takes one parameter - the variable x. The function itself\n",
    "# just squares x and returns the result, which is whatever the one line of code\n",
    "# evaluates to. In this way, lambda functions can make your code concise and Pythonic.\n",
    "\n",
    "# Write the lambda function using replace\n",
    "tips['total_dollar_replace'] = tips.total_dollar.apply(lambda x: x.replace('$', ''))\n",
    "\n",
    "# Write the lambda function using regular expressions\n",
    "tips['total_dollar_re'] = tips.total_dollar.apply(lambda x: re.findall('\\d+\\.\\d+', x)[0])\n",
    "\n",
    "# Print the head of tips\n",
    "print(tips.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the new DataFrame: tracks\n",
    "tracks = billboard[['year','artist','track','time']]\n",
    "\n",
    "# Print info of tracks\n",
    "print(tracks.info())\n",
    "\n",
    "# Drop the duplicates: tracks_no_duplicates\n",
    "tracks_no_duplicates = tracks.drop_duplicates()\n",
    "\n",
    "# Print info of tracks\n",
    "print(tracks_no_duplicates.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the mean of the Ozone column: oz_mean\n",
    "oz_mean = airquality.Ozone.mean()\n",
    "\n",
    "# Replace all the missing values in the Ozone column with the mean\n",
    "airquality['Ozone'] = airquality['Ozone'].fillna(oz_mean)\n",
    "\n",
    "# Print the info of airquality\n",
    "print(airquality.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the .all() method together with the .notnull() DataFrame method to check\n",
    "# for missing values in a column. The .all() method returns True if all values\n",
    "# are True. When used on a DataFrame, it returns a Series of Booleans - one for\n",
    "# each column in the DataFrame. So if you are using it on a DataFrame, like in\n",
    "# this exercise, you need to chain another .all() method so that you return only\n",
    "# one True or False value. When using these within an assert statement, nothing\n",
    "# will be returned if the assert statement is true: This is how you can confirm\n",
    "# that the data you are checking are valid.\n",
    "\n",
    "# Note: You can use pd.notnull(df) as an alternative to df.notnull().\n",
    "\n",
    "\n",
    "# The first .all() method will return a True or False for each column, while the second\n",
    "# .all() method will return a single True or False.\n",
    "# Assert that there are no missing values\n",
    "assert pd.notnull(ebola).all().all()\n",
    "\n",
    "# Assert that all values are >= 0\n",
    "assert (ebola >= 0).all().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
