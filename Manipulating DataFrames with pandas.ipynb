{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Read in filename and set the index: election\n",
    "election = pd.read_csv(filename, index_col ='county')\n",
    "\n",
    "# Create a separate dataframe with the columns ['winner', 'total', 'voters']: results\n",
    "results = election[['winner', 'total', 'voters']]\n",
    "\n",
    "# Print the output of results.head()\n",
    "print(results.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Slice the columns from the starting column to 'Obama': left_columns\n",
    "left_columns = election.loc[:,'state':'Obama']\n",
    "\n",
    "# Print the output of left_columns.head()\n",
    "print(left_columns.head())\n",
    "\n",
    "# Slice the columns from 'Obama' to 'winner': middle_columns\n",
    "middle_columns = election.loc[:,'Obama':'winner']\n",
    "\n",
    "# Print the output of middle_columns.head()\n",
    "print(middle_columns.head())\n",
    "\n",
    "# Slice the columns from 'Romney' to the end: 'right_columns'\n",
    "right_columns = election.loc[:,'Romney':]\n",
    "\n",
    "# Print the output of right_columns.head()\n",
    "print(right_columns.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the list of row labels: rows\n",
    "rows = ['Philadelphia', 'Centre', 'Fulton']\n",
    "\n",
    "# Create the list of column labels: cols\n",
    "cols = ['winner', 'Obama', 'Romney']\n",
    "\n",
    "# Create the new DataFrame: three_counties\n",
    "three_counties = election.loc[rows,cols]\n",
    "\n",
    "# Print the three_counties DataFrame\n",
    "print(three_counties)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the boolean array: high_turnout\n",
    "high_turnout = election.turnout > 70\n",
    "\n",
    "# Filter the election DataFrame with the high_turnout array: high_turnout_df\n",
    "high_turnout_df = election[high_turnout]\n",
    "\n",
    "# Print the high_turnout_results DataFrame\n",
    "print(high_turnout_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# Create the boolean array: too_close\n",
    "too_close = election.margin < 1\n",
    "\n",
    "# Assign np.nan to the 'winner' column where the results were too close to call\n",
    "election.winner[too_close] = np.nan\n",
    "\n",
    "# Print the output of election.info()\n",
    "print(election.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In certain scenarios, it may be necessary to remove rows and columns with\n",
    "# missing data from a DataFrame. The .dropna() method is used to perform this action. \n",
    "\n",
    "# Select the 'age' and 'cabin' columns: df\n",
    "df = titanic.loc[:,['age','cabin']]\n",
    "\n",
    "# Print the shape of df\n",
    "print(df.shape)\n",
    "\n",
    "# Drop rows in df with how='any' and print the shape\n",
    "print(df.dropna(how = 'any').shape)\n",
    "\n",
    "# Drop rows in df with how='all' and print the shape\n",
    "print(df.dropna(how = 'all').shape)\n",
    "\n",
    "# Call .dropna() with thresh=1000 and axis='columns' and print the output of .info() from titanic\n",
    "print(titanic.dropna(thresh=1000, axis='columns').info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The .apply() method can be used on a pandas DataFrame to apply an arbitrary Python function to every element. \n",
    "\n",
    "# Write a function to convert degrees Fahrenheit to degrees Celsius: to_celsius\n",
    "def to_celsius(F):\n",
    "    return 5/9*(F - 32)\n",
    "\n",
    "# Apply the function over 'Mean TemperatureF' and 'Mean Dew PointF': df_celsius\n",
    "df_celsius = weather.loc[:,['Mean TemperatureF', 'Mean Dew PointF']].apply(to_celsius)\n",
    "\n",
    "# Reassign the columns df_celsius\n",
    "df_celsius.columns = ['Mean TemperatureC', 'Mean Dew PointC']\n",
    "\n",
    "# Print the output of df_celsius.head()\n",
    "print(df_celsius.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The .map() method is used to transform values according to a Python dictionary look-up.\n",
    "# Create the dictionary: red_vs_blue\n",
    "red_vs_blue = {'Obama':'blue', 'Romney':'red'}\n",
    "\n",
    "# Use the dictionary to map the 'winner' column to the new column: election['color']\n",
    "election['color'] = election.winner.map(red_vs_blue)\n",
    "\n",
    "# Print the output of election.head()\n",
    "print(election.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# When performance is paramount, you should avoid using .apply() and .map()\n",
    "# because those constructs perform Python for-loops over the data stored in \n",
    "# a pandas Series or DataFrame. By using vectorized functions instead, you\n",
    "# can loop over the data at the same speed as compiled code (C, Fortran, etc.)!\n",
    "# NumPy, SciPy and pandas come with a variety of vectorized functions\n",
    "# (called Universal Functions or UFuncs in NumPy).\n",
    "\n",
    "# You can even write your own vectorized functions, but for now we will focus\n",
    "# on the ones distributed by NumPy and pandas.\n",
    "\n",
    "# In this exercise you're going to import the zscore method from scipy.stats and\n",
    "# use it to compute the deviation in voter turnout in Pennsylvania from the mean\n",
    "# in fractions of the standard deviation. In statistics, the z-score is the number\n",
    "# of standard deviations by which an observation is above the mean - so if it is\n",
    "# negative, it means the observation is below the mean.\n",
    "\n",
    "# Instead of using .apply() as you did in the earlier exercises, the zscore UFunc\n",
    "# will take a pandas Series as input and return a NumPy array. You will then assign\n",
    "# the values of the NumPy array to a new column in the DataFrame. \n",
    "\n",
    "\n",
    "# Import zscore from scipy.stats\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Call zscore with election['turnout'] as input: turnout_zscore\n",
    "turnout_zscore = zscore(election['turnout'])\n",
    "\n",
    "# Print the type of turnout_zscore\n",
    "print(type(turnout_zscore))\n",
    "\n",
    "# Assign turnout_zscore to a new column: election['turnout_zscore']\n",
    "election['turnout_zscore'] = turnout_zscore\n",
    "\n",
    "# Print the output of election.head()\n",
    "print(election.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Indexes are immutable objects. This means that if you want to change \n",
    "# or modify the index in a DataFrame, then you need to change the whole index. \n",
    "\n",
    "# A list comprehension is a succinct way to generate a list in one line.\n",
    "# For example, the following list comprehension generates a list that contains\n",
    "# the cubes of all numbers from 0 to 9: \n",
    "cubes = [i**3 for i in range(10)].\n",
    "#This is equivalent to the following code:\n",
    "\n",
    "cubes = []\n",
    "for i in range(10):\n",
    "    cubes.append(i**3)\n",
    "    \n",
    "# Create the list of new indexes: new_idx\n",
    "new_idx = [i.upper() for i in sales.index]\n",
    "\n",
    "# Assign new_idx to sales.index\n",
    "sales.index = new_idx\n",
    "\n",
    "# Print the sales DataFrame\n",
    "print(sales)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 8, 27, 64, 125, 216, 343, 512, 729]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assign the string 'MONTHS' to sales.index.name\n",
    "sales.index.name = 'MONTHS'\n",
    "\n",
    "# Print the sales DataFrame\n",
    "print(sales)\n",
    "\n",
    "# Assign the string 'PRODUCTS' to sales.columns.name \n",
    "sales.columns.name = 'PRODUCTS'\n",
    "\n",
    "# Print the sales dataframe again\n",
    "print(sales)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can also build the DataFrame and index independently, and then put them\n",
    "# together. If you take this route, be careful, as any mistakes in generating\n",
    "# the DataFrame or the index can cause the data and the index to be aligned incorrectly.\n",
    "\n",
    "# Generate the list of months: months\n",
    "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun']\n",
    "\n",
    "# Assign months to sales.index\n",
    "sales.index = months\n",
    "\n",
    "# Print the modified sales DataFrame\n",
    "print(sales)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print sales.loc[['CA', 'TX']]\n",
    "print(sales.loc[['CA','TX']])\n",
    "\n",
    "# Print sales['CA':'TX']\n",
    "print(sales['CA':'TX'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# With a MultiIndex, you should always ensure the index is sorted. You can\n",
    "# skip this only if you know the data is already sorted on the index fields.\n",
    "\n",
    "# Set the index to be the columns ['state', 'month']: sales\n",
    "sales = sales.set_index(['state','month'])\n",
    "\n",
    "# Sort the MultiIndex: sales\n",
    "sales = sales.sort_index()\n",
    "\n",
    "# Print the sales DataFrame\n",
    "print(sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the index to the column 'state': sales\n",
    "sales = sales.set_index('state')\n",
    "\n",
    "# Print the sales DataFrame\n",
    "print(sales)\n",
    "\n",
    "# Access the data from 'NY'\n",
    "print(sales.loc['NY'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Looking up data based on inner levels of a MultiIndex can be a bit trickier.\n",
    "# In this exercise, you will use your sales DataFrame to do some increasingly complex lookups.\n",
    "\n",
    "# The trickiest of all these lookups are when you want to access some inner levels of the\n",
    "# index. In this case, you need to use slice(None) in the slicing parameter for the outermost\n",
    "# dimension(s) instead of the usual :, or use pd.IndexSlice. You can refer to the pandas\n",
    "# documentation for more details. For example, in the video, Dhavide used the following\n",
    "# code to extract rows from all Symbols for the dates Oct. 3rd through 4th inclusive:\n",
    "\n",
    "# stocks.loc[(slice(None), slice('2016-10-03', '2016-10-04')), :]\n",
    "\n",
    "# Look up data for NY in month 1: NY_month1\n",
    "NY_month1 = sales.loc[('NY', 1), :]\n",
    "\n",
    "# Look up data for CA and TX in month 2: CA_TX_month2\n",
    "CA_TX_month2 = sales.loc[(['CA', 'TX'],2),:]\n",
    "\n",
    "# Look up data for all states in month 2: all_month2\n",
    "all_month2 = sales.loc[(slice(None), 2),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pivot the users DataFrame: visitors_pivot\n",
    "visitors_pivot = users.pivot(index = 'weekday', columns = 'city', values = 'visitors')\n",
    "\n",
    "# Print the pivoted DataFrame\n",
    "print(visitors_pivot)\n",
    "\n",
    "''' print(users)\n",
    "  weekday    city  visitors  signups\n",
    "0     Sun  Austin       139        7\n",
    "1     Sun  Dallas       237       12\n",
    "2     Mon  Austin       326        3\n",
    "3     Mon  Dallas       456        5\n",
    "\n",
    "<script.py> output:\n",
    "    city     Austin  Dallas\n",
    "    weekday                \n",
    "    Mon         326     456\n",
    "    Sun         139     237'\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If you do not select any particular variables, all of them will be pivoted. \n",
    "# In this case - with the users DataFrame - both 'visitors' and 'signups' will\n",
    "# be pivoted, creating hierarchical column labels.\n",
    "\n",
    "# Pivot users with signups indexed by weekday and city: signups_pivot\n",
    "signups_pivot = users.pivot(index = 'weekday', columns = 'city',values= 'signups')\n",
    "\n",
    "# Print signups_pivot\n",
    "print(signups_pivot)\n",
    "\n",
    "# Pivot users pivoted by both signups and visitors: pivot\n",
    "pivot = users.pivot(index = 'weekday', columns = 'city')\n",
    "\n",
    "# Print the pivoted DataFrame\n",
    "print(pivot)\n",
    "\n",
    "\n",
    "'''\n",
    " users\n",
    "Out[7]: \n",
    "  weekday    city  visitors  signups\n",
    "0     Sun  Austin       139        7\n",
    "1     Sun  Dallas       237       12\n",
    "2     Mon  Austin       326        3\n",
    "3     Mon  Dallas       456        5\n",
    "\n",
    "<script.py> output:\n",
    "    city     Austin  Dallas\n",
    "    weekday                \n",
    "    Mon           3       5\n",
    "    Sun           7      12\n",
    "            visitors        signups       \n",
    "    city      Austin Dallas  Austin Dallas\n",
    "    weekday                               \n",
    "    Mon          326    456       3      5\n",
    "    Sun          139    237       7     12\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Unstack users by 'weekday': byweekday\n",
    "byweekday = users.unstack(level = 'weekday')\n",
    "\n",
    "# Print the byweekday DataFrame\n",
    "print(byweekday)\n",
    "\n",
    "# Stack byweekday by 'weekday' and print it\n",
    "print(byweekday.stack(level = 'weekday'))\n",
    "\n",
    "'''\n",
    "<script.py> output:\n",
    "            visitors      signups    \n",
    "    weekday      Mon  Sun     Mon Sun\n",
    "    city                             \n",
    "    Austin       326  139       3   7\n",
    "    Dallas       456  237       5  12\n",
    "                    visitors  signups\n",
    "    city   weekday                   \n",
    "    Austin Mon           326        3\n",
    "           Sun           139        7\n",
    "    Dallas Mon           456        5\n",
    "           Sun           237       12\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Unstack users by 'city': bycity\n",
    "bycity = users.unstack(level = 'city')\n",
    "\n",
    "# Print the bycity DataFrame\n",
    "print(bycity)\n",
    "\n",
    "# Stack bycity by 'city' and print it\n",
    "print(bycity.stack(level = 'city'))\n",
    "\n",
    "'''\n",
    "<script.py> output:\n",
    "            visitors        signups       \n",
    "    city      Austin Dallas  Austin Dallas\n",
    "    weekday                               \n",
    "    Mon          326    456       3      5\n",
    "    Sun          139    237       7     12\n",
    "                    visitors  signups\n",
    "    weekday city                     \n",
    "    Mon     Austin       326        3\n",
    "            Dallas       456        5\n",
    "    Sun     Austin       139        7\n",
    "            Dallas       237       12\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Continuing from the previous exercise, you will now use .swaplevel(0, 1) to flip\n",
    "# the index levels. Note they won't be sorted. To sort them, you will have to follow\n",
    "# up with a .sort_index(). You will then obtain the original DataFrame. Note that an\n",
    "# unsorted index leads to slicing failures.\n",
    "\n",
    "# Stack 'city' back into the index of bycity: newusers\n",
    "newusers = bycity.stack(level = 'city')\n",
    "\n",
    "# Swap the levels of the index of newusers: newusers\n",
    "newusers = newusers.swaplevel(0,1)\n",
    "\n",
    "# Print newusers and verify that the index is not sorted\n",
    "print(newusers)\n",
    "\n",
    "# Sort the index of newusers: newusers\n",
    "newusers = newusers.sort_index()\n",
    "\n",
    "# Print newusers and verify that the index is now sorted\n",
    "print(newusers)\n",
    "\n",
    "# Verify that the new DataFrame is equal to the original\n",
    "print(newusers.equals(users))\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "<script.py> output:\n",
    "                    visitors  signups\n",
    "    city   weekday                   \n",
    "    Austin Mon           326        3\n",
    "    Dallas Mon           456        5\n",
    "    Austin Sun           139        7\n",
    "    Dallas Sun           237       12\n",
    "                    visitors  signups\n",
    "    city   weekday                   \n",
    "    Austin Mon           326        3\n",
    "           Sun           139        7\n",
    "    Dallas Mon           456        5\n",
    "           Sun           237       12\n",
    "    True\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the goal of melting is to restore a pivoted DataFrame to its original form,\n",
    "# or to change it from a wide shape to a long shape. You can explicitly specify\n",
    "# the columns that should remain in the reshaped DataFrame with id_vars, and\n",
    "# list which columns to convert into values with value_vars. As Dhavide demonstrated,\n",
    "# if you don't pass a name to the values in pd.melt(), you will lose the name of your\n",
    "# variable. You can fix this by using the value_name keyword argument.\n",
    "\n",
    "'''\n",
    "In [1]: visitors_by_city_weekday\n",
    "Out[1]: \n",
    "city     Austin  Dallas\n",
    "weekday                \n",
    "Mon         326     456\n",
    "Sun         139     237\n",
    "'''\n",
    "\n",
    "\n",
    "# Reset the index: visitors_by_city_weekday\n",
    "visitors_by_city_weekday = visitors_by_city_weekday.reset_index() \n",
    "\n",
    "# Print visitors_by_city_weekday\n",
    "print(visitors_by_city_weekday)\n",
    "\n",
    "# Melt visitors_by_city_weekday: visitors\n",
    "visitors = pd.melt(visitors_by_city_weekday, id_vars=['weekday'], value_name= 'visitors')\n",
    "\n",
    "# Print visitors\n",
    "print(visitors)\n",
    "\n",
    "'''\n",
    "<script.py> output:\n",
    "    city weekday  Austin  Dallas\n",
    "    0        Mon     326     456\n",
    "    1        Sun     139     237\n",
    "      weekday    city  visitors\n",
    "    0     Mon  Austin       326\n",
    "    1     Sun  Austin       139\n",
    "    2     Mon  Dallas       456\n",
    "    3     Sun  Dallas       237\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "In [1]: users\n",
    "Out[1]: \n",
    "  weekday    city  visitors  signups\n",
    "0     Sun  Austin       139        7\n",
    "1     Sun  Dallas       237       12\n",
    "2     Mon  Austin       326        3\n",
    "3     Mon  Dallas       456        5\n",
    "'''\n",
    "\n",
    "# You can move multiple columns into a single column (making the data long and skinny) by \"melting\" multiple columns. \n",
    "\n",
    "# Melt users: skinny\n",
    "skinny = pd.melt(users, id_vars = ['weekday','city'])\n",
    "\n",
    "# Print skinny\n",
    "print(skinny)\n",
    "\n",
    "'''\n",
    "<script.py> output:\n",
    "      weekday    city  variable  value\n",
    "    0     Sun  Austin  visitors    139\n",
    "    1     Sun  Dallas  visitors    237\n",
    "    2     Mon  Austin  visitors    326\n",
    "    3     Mon  Dallas  visitors    456\n",
    "    4     Sun  Austin   signups      7\n",
    "    5     Sun  Dallas   signups     12\n",
    "    6     Mon  Austin   signups      3\n",
    "    7     Mon  Dallas   signups      5\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sometimes, all you need is some key-value pairs, and the context does not matter.\n",
    "# If said context is in the index, you can easily obtain what you want. For example,\n",
    "# in the users DataFrame, the visitors and signups columns lend themselves well to being\n",
    "# represented as key-value pairs. So if you created a hierarchical index with 'city' and\n",
    "# 'weekday' columns as the index, you can easily extract key-value pairs for the 'visitors'\n",
    "# and 'signups' columns by melting users and specifying col_level=0\n",
    "\n",
    "# Set the new index: users_idx\n",
    "users_idx = users.set_index(['city', 'weekday'])\n",
    "\n",
    "# Print the users_idx DataFrame\n",
    "print(users_idx)\n",
    "\n",
    "# Obtain the key-value pairs: kv_pairs\n",
    "kv_pairs = pd.melt(users_idx, col_level = 0)\n",
    "\n",
    "# Print the key-value pairs\n",
    "print(kv_pairs)\n",
    "\n",
    "\n",
    "'''\n",
    "<script.py> output:\n",
    "                    visitors  signups\n",
    "    city   weekday                   \n",
    "    Austin Sun           139        7\n",
    "    Dallas Sun           237       12\n",
    "    Austin Mon           326        3\n",
    "    Dallas Mon           456        5\n",
    "       variable  value\n",
    "    0  visitors    139\n",
    "    1  visitors    237\n",
    "    2  visitors    326\n",
    "    3  visitors    456\n",
    "    4   signups      7\n",
    "    5   signups     12\n",
    "    6   signups      3\n",
    "    7   signups      5\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a pivot table allows you to see all of your variables as a function of two other variables.\n",
    "# In this exercise, you will use the .pivot_table() method to see how the users DataFrame\n",
    "# entries appear when presented as functions of the 'weekday' and 'city' columns. That is,\n",
    "# with the rows indexed by 'weekday' and the columns indexed by 'city'.\n",
    "\n",
    "'''\n",
    "In [1]: users\n",
    "Out[1]: \n",
    "  weekday    city  visitors  signups\n",
    "0     Sun  Austin       139        7\n",
    "1     Sun  Dallas       237       12\n",
    "2     Mon  Austin       326        3\n",
    "3     Mon  Dallas       456        5\n",
    "'''\n",
    "\n",
    "# Create the DataFrame with the appropriate pivot table: by_city_day\n",
    "by_city_day = users.pivot_table(index = 'weekday', columns = 'city')\n",
    "\n",
    "# Print by_city_day\n",
    "print(by_city_day)\n",
    "\n",
    "\n",
    "'''\n",
    "<script.py> output:\n",
    "            visitors        signups       \n",
    "    city      Austin Dallas  Austin Dallas\n",
    "    weekday                               \n",
    "    Mon          326    456       3      5\n",
    "    Sun          139    237       7     12\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can also use aggregation functions with in a pivot table by specifying the aggfunc parameter.\n",
    "\n",
    "'''\n",
    "In [1]: users\n",
    "Out[1]: \n",
    "  weekday    city  visitors  signups\n",
    "0     Sun  Austin       139        7\n",
    "1     Sun  Dallas       237       12\n",
    "2     Mon  Austin       326        3\n",
    "3     Mon  Dallas       456        5\n",
    "'''\n",
    "\n",
    "# Use a pivot table to display the count of each column: count_by_weekday1\n",
    "count_by_weekday1 = users.pivot_table(index = 'weekday', aggfunc = 'count')\n",
    "\n",
    "# Print count_by_weekday\n",
    "print(count_by_weekday1)\n",
    "\n",
    "# Replace 'aggfunc='count'' with 'aggfunc=len': count_by_weekday2\n",
    "count_by_weekday2 = users.pivot_table(index = 'weekday', aggfunc = len)\n",
    "\n",
    "# Verify that the same result is obtained\n",
    "print('==========================================')\n",
    "print(count_by_weekday1.equals(count_by_weekday2))\n",
    "\n",
    "\n",
    "'''\n",
    "<script.py> output:\n",
    "            visitors        signups       \n",
    "    city      Austin Dallas  Austin Dallas\n",
    "    weekday                               \n",
    "    Mon            1      1       1      1\n",
    "    Sun            1      1       1      1\n",
    "    ==========================================\n",
    "    True\n",
    "\n",
    "<script.py> output:\n",
    "             city  signups  visitors\n",
    "    weekday                         \n",
    "    Mon         2        2         2\n",
    "    Sun         2        2         2\n",
    "    ==========================================\n",
    "    True\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sometimes it's useful to add totals in the margins of a pivot table. You can do this with the argument margins=True\n",
    "\n",
    "'''\n",
    "In [1]: users\n",
    "Out[1]: \n",
    "  weekday    city  visitors  signups\n",
    "0     Sun  Austin       139        7\n",
    "1     Sun  Dallas       237       12\n",
    "2     Mon  Austin       326        3\n",
    "3     Mon  Dallas       456        5\n",
    "'''\n",
    "\n",
    "# Create the DataFrame with the appropriate pivot table: signups_and_visitors\n",
    "signups_and_visitors = users.pivot_table(index = 'weekday', aggfunc = sum)\n",
    "\n",
    "# Print signups_and_visitors\n",
    "print(signups_and_visitors)\n",
    "\n",
    "# Add in the margins: signups_and_visitors_total \n",
    "signups_and_visitors_total = users.pivot_table(index = 'weekday', aggfunc = sum, margins = True)\n",
    "\n",
    "# Print signups_and_visitors_total\n",
    "print(signups_and_visitors_total)\n",
    "\n",
    "\n",
    "'''\n",
    "<script.py> output:\n",
    "             signups  visitors\n",
    "    weekday                   \n",
    "    Mon            8       782\n",
    "    Sun           19       376\n",
    "             signups  visitors\n",
    "    weekday                   \n",
    "    Mon          8.0     782.0\n",
    "    Sun         19.0     376.0\n",
    "    All         27.0    1158.0\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Group titanic by 'pclass'\n",
    "by_class = titanic.groupby('pclass')\n",
    "\n",
    "# Aggregate 'survived' column of by_class by count\n",
    "count_by_class = by_class['survived'].count()\n",
    "\n",
    "# Print count_by_class\n",
    "print(count_by_class)\n",
    "\n",
    "# Group titanic by 'embarked' and 'pclass'\n",
    "by_mult = titanic.groupby(['embarked','pclass'])\n",
    "\n",
    "# Aggregate 'survived' column of by_mult by count\n",
    "count_mult = by_mult['survived'].count()\n",
    "\n",
    "# Print count_mult\n",
    "print(count_mult)\n",
    "\n",
    "\n",
    "'''\n",
    "<script.py> output:\n",
    "    pclass\n",
    "    1    323\n",
    "    2    277\n",
    "    3    709\n",
    "    Name: survived, dtype: int64\n",
    "    embarked  pclass\n",
    "    C         1         141\n",
    "              2          28\n",
    "              3         101\n",
    "    Q         1           3\n",
    "              2           7\n",
    "              3         113\n",
    "    S         1         177\n",
    "              2         242\n",
    "              3         495\n",
    "    Name: survived, dtype: int64\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "In [1]: life_fname\n",
    "Out[1]: 'https://s3.amazonaws.com/assets.datacamp.com/production/course_1650/datasets/life_expectancy.csv'\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "In [4]: regions_fname\n",
    "Out[4]: 'https://s3.amazonaws.com/assets.datacamp.com/production/course_1650/datasets/regions.csv'\n",
    "'''\n",
    "\n",
    "\n",
    "# Read life_fname into a DataFrame: life\n",
    "life = pd.read_csv(life_fname, index_col='Country')\n",
    "\n",
    "# Read regions_fname into a DataFrame: regions\n",
    "regions = pd.read_csv(regions_fname, index_col = 'Country')\n",
    "\n",
    "# Group life by regions['region']: life_by_region\n",
    "life_by_region = life.groupby(regions.region)\n",
    "\n",
    "# Print the mean over the '2010' column of life_by_region\n",
    "print(life_by_region['2010'].mean())\n",
    "\n",
    "'''\n",
    "<script.py> output:\n",
    "    region\n",
    "    America                       74.037350\n",
    "    East Asia & Pacific           73.405750\n",
    "    Europe & Central Asia         75.656387\n",
    "    Middle East & North Africa    72.805333\n",
    "    South Asia                    68.189750\n",
    "    Sub-Saharan Africa            57.575080\n",
    "    Name: 2010, dtype: float64\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The .agg() method can be used with a tuple or list of aggregations as input. \n",
    "# When applying multiple aggregations on multiple columns, the aggregated DataFrame has a multi-level column index.\n",
    "\n",
    "# Group titanic by 'pclass': by_class\n",
    "by_class = titanic.groupby('pclass')\n",
    "\n",
    "# Select 'age' and 'fare'\n",
    "by_class_sub = by_class[['age','fare']]\n",
    "\n",
    "# Aggregate by_class_sub by 'max' and 'median': aggregated\n",
    "aggregated = by_class_sub.agg(['max', 'median'])\n",
    "\n",
    "# Print the maximum age in each class\n",
    "print(aggregated.loc[:, ('age','max')])\n",
    "\n",
    "# Print the median fare in each class\n",
    "print(aggregated.loc[:,('fare', 'median')])\n",
    "\n",
    "'''\n",
    "<script.py> output:\n",
    "    pclass\n",
    "    1    80.0\n",
    "    2    70.0\n",
    "    3    74.0\n",
    "    Name: (age, max), dtype: float64\n",
    "    pclass\n",
    "    1    60.0000\n",
    "    2    15.0458\n",
    "    3     8.0500\n",
    "    Name: (fare, median), dtype: float64\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If you have a DataFrame with a multi-level row index, the individual levels can\n",
    "# be used to perform the groupby. This allows advanced aggregation techniques to be\n",
    "# applied along one or more levels in the index and across one or more columns.\n",
    "\n",
    "# Read the CSV file into a DataFrame and sort the index: gapminder\n",
    "gapminder = pd.read_csv('gapminder.csv', index_col = ['Year', 'region', 'Country']).sort_index()\n",
    "\n",
    "# Group gapminder by 'Year' and 'region': by_year_region\n",
    "by_year_region = gapminder.groupby(level = ['Year', 'region'])\n",
    "\n",
    "# Define the function to compute spread: spread\n",
    "def spread(series):\n",
    "    return series.max() - series.min()\n",
    "\n",
    "# Create the dictionary: aggregator\n",
    "aggregator = {'population':'sum', 'child_mortality':'mean', 'gdp':spread}\n",
    "\n",
    "# Aggregate by_year_region using the dictionary: aggregated\n",
    "aggregated = by_year_region.agg(aggregator)\n",
    "\n",
    "# Print the last 6 entries of aggregated \n",
    "print(aggregated.tail(6))\n",
    "\n",
    "\n",
    "'''\n",
    "<script.py> output:\n",
    "                                          gdp  child_mortality    population\n",
    "    Year region                                                             \n",
    "    2013 America                      49634.0        17.745833  9.629087e+08\n",
    "         East Asia & Pacific         134744.0        22.285714  2.244209e+09\n",
    "         Europe & Central Asia        86418.0         9.831875  8.968788e+08\n",
    "         Middle East & North Africa  128676.0        20.221500  4.030504e+08\n",
    "         South Asia                   11469.0        46.287500  1.701241e+09\n",
    "         Sub-Saharan Africa           32035.0        76.944490  9.205996e+08\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Groubpy operations can also be performed on transformations of the index values.\n",
    "# In the case of a DateTimeIndex, we can extract portions of the datetime over which to group.\n",
    "\n",
    "# Is there a day of the week that is more popular for customers? To find out, you're going to use .strftime('%a')\n",
    "# to transform the index datetime values to abbreviated days of the week.\n",
    "\n",
    "# Read file: sales\n",
    "sales = pd.read_csv('sales.csv', index_col = 'Date', parse_dates = True)\n",
    "\n",
    "# Create a groupby object: by_day\n",
    "by_day = sales.groupby(sales.index.strftime('%a'))\n",
    "\n",
    "# Create sum: units_sum\n",
    "units_sum = by_day['Units'].sum()\n",
    "\n",
    "# Print units_sum\n",
    "print(units_sum)\n",
    "\n",
    "\n",
    "'''\n",
    "<script.py> output:\n",
    "    Mon    48\n",
    "    Sat     7\n",
    "    Thu    59\n",
    "    Tue    13\n",
    "    Wed    48\n",
    "    Name: Units, dtype: int64\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you can apply a .transform() method after grouping to apply a function\n",
    "# to groups of data independently. The z-score is also useful to find outliers:\n",
    "# a z-score value of +/- 3 is generally considered to be an outlier.\n",
    "\n",
    "# Import zscore\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Group gapminder_2010: standardized\n",
    "standardized = gapminder_2010.groupby('region')['life','fertility'].transform(zscore)\n",
    "\n",
    "# Construct a Boolean Series to identify outliers: outliers\n",
    "outliers = (standardized['life'] < -3) | (standardized['fertility'] > 3)\n",
    "\n",
    "# Filter gapminder_2010 by the outliers: gm_outliers\n",
    "gm_outliers = gapminder_2010.loc[outliers]\n",
    "\n",
    "# Print gm_outliers\n",
    "print(gm_outliers)\n",
    "\n",
    "'''\n",
    "<script.py> output:\n",
    "                 fertility    life  population  child_mortality     gdp  \\\n",
    "    Country                                                               \n",
    "    Guatemala        3.974  71.100  14388929.0             34.5  6849.0   \n",
    "    Haiti            3.350  45.000   9993247.0            208.8  1518.0   \n",
    "    Tajikistan       3.780  66.830   6878637.0             52.6  2110.0   \n",
    "    Timor-Leste      6.237  65.952   1124355.0             63.8  1777.0   \n",
    "    \n",
    "                                region  \n",
    "    Country                             \n",
    "    Guatemala                  America  \n",
    "    Haiti                      America  \n",
    "    Tajikistan   Europe & Central Asia  \n",
    "    Timor-Leste    East Asia & Pacific\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Many statistical and machine learning packages cannot determine the best action\n",
    "# to take when missing data entries are encountered. Dealing with missing data is\n",
    "# natural in pandas (both in using the default behavior and in defining a custom behavior).\n",
    "# In Chapter 1, you practiced using the .dropna() method to drop missing values. Now,\n",
    "# you will practice imputing missing values. You can use .groupby() and .transform()\n",
    "# to fill missing data appropriately for each group.\n",
    "\n",
    "# Create a groupby object: by_sex_class\n",
    "by_sex_class = titanic.groupby(['sex', 'pclass'])\n",
    "\n",
    "# Write a function that imputes median\n",
    "def impute_median(series):\n",
    "    return series.fillna(series.median())\n",
    "\n",
    "# Impute age and assign to titanic['age']\n",
    "titanic.age = by_sex_class['age'].transform(impute_median)\n",
    "\n",
    "# Print the output of titanic.tail(10)\n",
    "print(titanic.tail(10))\n",
    "\n",
    "\n",
    "'''\n",
    "<script.py> output:\n",
    "          pclass  survived                                     name     sex   age  \\\n",
    "    1299       3         0                      Yasbeck, Mr. Antoni    male  27.0   \n",
    "    1300       3         1  Yasbeck, Mrs. Antoni (Selini Alexander)  female  15.0   \n",
    "    1301       3         0                     Youseff, Mr. Gerious    male  45.5   \n",
    "    1302       3         0                        Yousif, Mr. Wazli    male  25.0   \n",
    "    1303       3         0                    Yousseff, Mr. Gerious    male  25.0   \n",
    "    1304       3         0                     Zabour, Miss. Hileni  female  14.5   \n",
    "    1305       3         0                    Zabour, Miss. Thamine  female  22.0   \n",
    "    1306       3         0                Zakarian, Mr. Mapriededer    male  26.5   \n",
    "    1307       3         0                      Zakarian, Mr. Ortin    male  27.0   \n",
    "    1308       3         0                       Zimmerman, Mr. Leo    male  29.0   \n",
    "    \n",
    "          sibsp  parch  ticket     fare cabin embarked boat   body home.dest  \n",
    "    1299      1      0    2659  14.4542   NaN        C    C    NaN       NaN  \n",
    "    1300      1      0    2659  14.4542   NaN        C  NaN    NaN       NaN  \n",
    "    1301      0      0    2628   7.2250   NaN        C  NaN  312.0       NaN  \n",
    "    1302      0      0    2647   7.2250   NaN        C  NaN    NaN       NaN  \n",
    "    1303      0      0    2627  14.4583   NaN        C  NaN    NaN       NaN  \n",
    "    1304      1      0    2665  14.4542   NaN        C  NaN  328.0       NaN  \n",
    "    1305      1      0    2665  14.4542   NaN        C  NaN    NaN       NaN  \n",
    "    1306      0      0    2656   7.2250   NaN        C  NaN  304.0       NaN  \n",
    "    1307      0      0    2670   7.2250   NaN        C  NaN    NaN       NaN  \n",
    "    1308      0      0  315082   7.8750   NaN        S  NaN    NaN       NaN\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The .apply() method when used on a groupby object performs an arbitrary function on \n",
    "# each of the groups. These functions can be aggregations, transformations or more complex\n",
    "# workflows. The .apply() method will then combine the results in an intelligent way.\n",
    "\n",
    "def disparity(gr):\n",
    "    # Compute the spread of gr['gdp']: s\n",
    "    s = gr['gdp'].max() - gr['gdp'].min()\n",
    "    # Compute the z-score of gr['gdp'] as (gr['gdp']-gr['gdp'].mean())/gr['gdp'].std(): z\n",
    "    z = (gr['gdp'] - gr['gdp'].mean())/gr['gdp'].std()\n",
    "    # Return a DataFrame with the inputs {'z(gdp)':z, 'regional spread(gdp)':s}\n",
    "    return pd.DataFrame({'z(gdp)':z , 'regional spread(gdp)':s})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Group gapminder_2010 by 'region': regional\n",
    "regional = gapminder_2010.groupby('region')\n",
    "\n",
    "# Apply the disparity function on regional: reg_disp\n",
    "reg_disp = regional.apply(disparity)\n",
    "\n",
    "# Print the disparity of 'United States', 'United Kingdom', and 'China'\n",
    "print(reg_disp.loc[['United States', 'United Kingdom', 'China'],:])\n",
    "\n",
    "\n",
    "'''\n",
    "<script.py> output:\n",
    "                    regional spread(gdp)    z(gdp)\n",
    "    Country                                       \n",
    "    United States                47855.0  3.013374\n",
    "    United Kingdom               89037.0  0.572873\n",
    "    China                        96993.0 -0.432756\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# By using .apply(), you can write functions that filter rows within groups.\n",
    "# The .apply() method will handle the iteration over individual groups and then\n",
    "# re-combine them back into a Series or DataFrame.\n",
    "\n",
    "# In this exercise you'll take the Titanic data set and analyze survival rates\n",
    "# from the 'C' deck, which contained the most passengers. To do this you'll group\n",
    "# the dataset by 'sex' and then use the .apply() method on a provided user defined\n",
    "# function which calculates the mean survival rates on the 'C' deck:\n",
    "\n",
    "def c_deck_survival(gr):\n",
    "\n",
    "    c_passengers = gr['cabin'].str.startswith('C').fillna(False)\n",
    "\n",
    "    return gr.loc[c_passengers, 'survived'].mean()\n",
    "\n",
    "# Create a groupby object using titanic over the 'sex' column: by_sex\n",
    "by_sex = titanic.groupby('sex')\n",
    "\n",
    "# Call by_sex.apply with the function c_deck_survival and print the result\n",
    "c_surv_by_sex = by_sex.apply(c_deck_survival)\n",
    "\n",
    "# Print the survival rates\n",
    "print(c_surv_by_sex)\n",
    "\n",
    "\n",
    "'''\n",
    "<script.py> output:\n",
    "    sex\n",
    "    female    0.913043\n",
    "    male      0.312500\n",
    "    dtype: float64\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can use groupby with the .filter() method to remove whole groups of rows\n",
    "# from a DataFrame based on a boolean condition.\n",
    "\n",
    "# In this exercise, you'll take the February sales data and remove entries from\n",
    "# companies that purchased less than 35 Units in the whole month.\n",
    "\n",
    "# First, you'll identify how many units each company bought for verification. Next\n",
    "# you'll use the .filter() method after grouping by 'Company' to remove all rows\n",
    "# belonging to companies whose sum over the 'Units' column was less than 35. Finally,\n",
    "# verify that the three companies whose total Units purchased were less than 35 have\n",
    "# been filtered out from the DataFrame.\n",
    "\n",
    "# Read the CSV file into a DataFrame: sales\n",
    "sales = pd.read_csv('sales.csv', index_col='Date', parse_dates=True)\n",
    "\n",
    "# Group sales by 'Company': by_company\n",
    "by_company = sales.groupby('Company')\n",
    "\n",
    "# Compute the sum of the 'Units' of by_company: by_com_sum\n",
    "by_com_sum = by_company['Units'].sum()\n",
    "print(by_com_sum)\n",
    "\n",
    "# Filter 'Units' where the sum is > 35: by_com_filt\n",
    "by_com_filt = by_company.filter(lambda g:g['Units'].sum() > 35)\n",
    "print(by_com_filt)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "<script.py> output:\n",
    "    Company\n",
    "    Acme Coporation    34\n",
    "    Hooli              30\n",
    "    Initech            30\n",
    "    Mediacore          45\n",
    "    Streeplex          36\n",
    "    Name: Units, dtype: int64\n",
    "                           Company   Product  Units\n",
    "    Date                                           \n",
    "    2015-02-02 21:00:00  Mediacore  Hardware      9\n",
    "    2015-02-04 15:30:00  Streeplex  Software     13\n",
    "    2015-02-09 09:00:00  Streeplex   Service     19\n",
    "    2015-02-09 13:00:00  Mediacore  Software      7\n",
    "    2015-02-19 11:00:00  Mediacore  Hardware     16\n",
    "    2015-02-19 16:00:00  Mediacore   Service     10\n",
    "    2015-02-21 05:00:00  Mediacore  Software      3\n",
    "    2015-02-26 09:00:00  Streeplex   Service      4\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You have seen how to group by a column, or by multiple columns. Sometimes,\n",
    "# you may instead want to group by a function/transformation of a column. The\n",
    "# key here is that the Series is indexed the same way as the DataFrame. You can\n",
    "# also mix and match column grouping with Series grouping.\n",
    "\n",
    "# In this exercise your job is to investigate survival rates of passengers on the\n",
    "# Titanic by 'age' and 'pclass'. In particular, the goal is to find out what fraction\n",
    "# of children under 10 survived in each 'pclass'. You'll do this by first creating a\n",
    "# boolean array where True is passengers under 10 years old and False is passengers over\n",
    "# 10. You'll use .map() to change these values to strings.\n",
    "\n",
    "# Finally, you'll group by the under 10 series and the 'pclass' column and aggregate the\n",
    "# 'survived' column. The 'survived' column has the value 1 if the passenger survived and\n",
    "# 0 otherwise. The mean of the 'survived' column is the fraction of passengers who lived.\n",
    "\n",
    "# Create the Boolean Series: under10\n",
    "under10 = (titanic['age'] < 10).map({True :'under 10',False: 'over 10'})\n",
    "\n",
    "# Group by under10 and compute the survival rate\n",
    "survived_mean_1 = titanic.groupby(under10)['survived'].mean()\n",
    "print(survived_mean_1)\n",
    "\n",
    "# Group by under10 and pclass and compute the survival rate\n",
    "survived_mean_2 = titanic.groupby([under10,'pclass'])['survived'].mean()\n",
    "print(survived_mean_2)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "<script.py> output:\n",
    "    age\n",
    "    over 10     0.366748\n",
    "    under 10    0.609756\n",
    "    Name: survived, dtype: float64\n",
    "    age       pclass\n",
    "    over 10   1         0.617555\n",
    "              2         0.380392\n",
    "              3         0.238897\n",
    "    under 10  1         0.750000\n",
    "              2         1.000000\n",
    "              3         0.446429\n",
    "    Name: survived, dtype: float64\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Notice that .value_counts() sorts by values by default. The result is returned as\n",
    "# a Series of counts indexed by unique entries from the original Series with values\n",
    "# (counts) ranked in descending order.\n",
    "\n",
    "# Select the 'NOC' column of medals: country_names\n",
    "country_names = medals['NOC']\n",
    "\n",
    "# Count the number of medals won by each country: medal_counts\n",
    "medal_counts = country_names.value_counts()\n",
    "\n",
    "# Print top 15 countries ranked by medals\n",
    "print(medal_counts.head(15))\n",
    "\n",
    "'''\n",
    "<script.py> output:\n",
    "    USA    4335\n",
    "    URS    2049\n",
    "    GBR    1594\n",
    "    FRA    1314\n",
    "    ITA    1228\n",
    "    GER    1211\n",
    "    AUS    1075\n",
    "    HUN    1053\n",
    "    SWE    1021\n",
    "    GDR     825\n",
    "    NED     782\n",
    "    JPN     704\n",
    "    CHN     679\n",
    "    RUS     638\n",
    "    ROU     624\n",
    "    Name: NOC, dtype: int64\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Rather than ranking countries by total medals won and showing that list, you may\n",
    "# want to see a bit more detail. You can use a pivot table to compute how many separate\n",
    "# bronze, silver and gold medals each country won. That pivot table can then be used to\n",
    "# repeat the previous computation to rank by total medals won.\n",
    "\n",
    "# Construct the pivot table: counted\n",
    "counted = medals.pivot_table(index = 'NOC', columns = 'Medal', values = 'Athlete',aggfunc= 'count')\n",
    "\n",
    "# Create the new column: counted['totals']\n",
    "counted['totals'] = counted.sum(axis = 'columns')\n",
    "\n",
    "# Sort counted by the 'totals' column\n",
    "counted = counted.sort_values('totals', ascending = False)\n",
    "\n",
    "# Print the top 15 rows of counted\n",
    "print(counted.head(15))\n",
    "\n",
    "\n",
    "'''\n",
    "<script.py> output:\n",
    "    Medal  Bronze    Gold  Silver  totals\n",
    "    NOC                                  \n",
    "    USA    1052.0  2088.0  1195.0  4335.0\n",
    "    URS     584.0   838.0   627.0  2049.0\n",
    "    GBR     505.0   498.0   591.0  1594.0\n",
    "    FRA     475.0   378.0   461.0  1314.0\n",
    "    ITA     374.0   460.0   394.0  1228.0\n",
    "    GER     454.0   407.0   350.0  1211.0\n",
    "    AUS     413.0   293.0   369.0  1075.0\n",
    "    HUN     345.0   400.0   308.0  1053.0\n",
    "    SWE     325.0   347.0   349.0  1021.0\n",
    "    GDR     225.0   329.0   271.0   825.0\n",
    "    NED     320.0   212.0   250.0   782.0\n",
    "    JPN     270.0   206.0   228.0   704.0\n",
    "    CHN     193.0   234.0   252.0   679.0\n",
    "    RUS     240.0   192.0   206.0   638.0\n",
    "    ROU     282.0   155.0   187.0   624.0\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select columns: ev_gen\n",
    "ev_gen = medals[['Event_gender','Gender']]\n",
    "\n",
    "# Drop duplicate pairs: ev_gen_uniques\n",
    "ev_gen_uniques = ev_gen.drop_duplicates()\n",
    "\n",
    "# Print ev_gen_uniques\n",
    "print(ev_gen_uniques)\n",
    "\n",
    "'''\n",
    "<script.py> output:\n",
    "          Event_gender Gender\n",
    "    0                M    Men\n",
    "    348              X    Men\n",
    "    416              W  Women\n",
    "    639              X  Women\n",
    "    23675            W    Men\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Group medals by the two columns: medals_by_gender\n",
    "medals_by_gender = medals.groupby(['Event_gender','Gender'])\n",
    "\n",
    "# Create a DataFrame with a group count: medal_count_by_gender\n",
    "medal_count_by_gender = medals_by_gender.count()\n",
    "\n",
    "# Print medal_count_by_gender\n",
    "print(medal_count_by_gender)\n",
    "\n",
    "'''\n",
    "                         Medal  \n",
    "    Event_gender Gender         \n",
    "    M            Men     20067  \n",
    "    W            Men         1  \n",
    "                 Women    7277  \n",
    "    X            Men      1653  \n",
    "                 Women     218\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the Boolean Series: sus\n",
    "sus = (medals.Event_gender == 'W') & (medals.Gender == 'Men')\n",
    "\n",
    "# Create a DataFrame with the suspicious row: suspect\n",
    "suspect = medals.loc[sus]\n",
    "\n",
    "# Print suspect\n",
    "print(suspect)\n",
    "\n",
    "\n",
    "'''\n",
    "<script.py> output:\n",
    "             City  Edition      Sport Discipline            Athlete  NOC Gender  \\\n",
    "    23675  Sydney     2000  Athletics  Athletics  CHEPCHUMBA, Joyce  KEN    Men   \n",
    "    \n",
    "              Event Event_gender   Medal  \n",
    "    23675  marathon            W  Bronze\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You may want to know which countries won medals in the most distinct sports.\n",
    "# The .nunique() method is the principal aggregation here. Given a categorical Series S,\n",
    "# S.nunique() returns the number of distinct categories.\n",
    "\n",
    "# Group medals by 'NOC': country_grouped\n",
    "country_grouped = medals.groupby('NOC')\n",
    "\n",
    "# Compute the number of distinct sports in which each country won medals: Nsports\n",
    "Nsports = country_grouped['Sport'].nunique()\n",
    "\n",
    "# Sort the values of Nsports in descending order\n",
    "Nsports = Nsports.sort_values(ascending = False)\n",
    "\n",
    "# Print the top 15 rows of Nsports\n",
    "print(Nsports.head(15))\n",
    "\n",
    "'''\n",
    "<script.py> output:\n",
    "    NOC\n",
    "    USA    34\n",
    "    GBR    31\n",
    "    FRA    28\n",
    "    GER    26\n",
    "    CHN    24\n",
    "    AUS    22\n",
    "    ESP    22\n",
    "    CAN    22\n",
    "    SWE    21\n",
    "    URS    21\n",
    "    ITA    21\n",
    "    NED    20\n",
    "    RUS    20\n",
    "    JPN    20\n",
    "    DEN    19\n",
    "    Name: Sport, dtype: int64\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract all rows for which the 'Edition' is between 1952 & 1988: during_cold_war\n",
    "during_cold_war = (medals.Edition >= 1952) & (medals.Edition <= 1988)\n",
    "\n",
    "# Extract rows for which 'NOC' is either 'USA' or 'URS': is_usa_urs\n",
    "is_usa_urs = medals.NOC.isin(['USA','URS'])\n",
    "\n",
    "# Use during_cold_war and is_usa_urs to create the DataFrame: cold_war_medals\n",
    "cold_war_medals = medals.loc[during_cold_war & is_usa_urs]\n",
    "\n",
    "# Group cold_war_medals by 'NOC'\n",
    "country_grouped = cold_war_medals.groupby('NOC')\n",
    "\n",
    "# Create Nsports\n",
    "Nsports = country_grouped.Sport.nunique().sort_values(ascending = False)\n",
    "\n",
    "# Print Nsports\n",
    "print(Nsports)\n",
    "\n",
    "\n",
    "'''\n",
    "<script.py> output:\n",
    "    NOC\n",
    "    URS    21\n",
    "    USA    20\n",
    "    Name: Sport, dtype: int64\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the pivot table: medals_won_by_country\n",
    "medals_won_by_country = medals.pivot_table(index = 'Edition', columns = 'NOC', values = 'Athlete', aggfunc = 'count')\n",
    "\n",
    "# Slice medals_won_by_country: cold_war_usa_usr_medals\n",
    "cold_war_usa_usr_medals = medals_won_by_country.loc[1952:1988, ['USA','URS']]\n",
    "\n",
    "# Create most_medals \n",
    "most_medals = cold_war_usa_usr_medals.idxmax(axis = 'columns')\n",
    "\n",
    "# Print most_medals.value_counts()\n",
    "print(most_medals.value_counts())\n",
    "\n",
    "'''\n",
    "<script.py> output:\n",
    "    URS    8\n",
    "    USA    2\n",
    "    dtype: int64\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the DataFrame: usa\n",
    "usa = medals.loc[medals.NOC == 'USA']\n",
    "\n",
    "# Group usa by ['Edition', 'Medal'] and aggregate over 'Athlete'\n",
    "usa_medals_by_year = usa.groupby(['Edition', 'Medal']).Athlete.count()\n",
    "\n",
    "# Reshape usa_medals_by_year by unstacking\n",
    "usa_medals_by_year = usa_medals_by_year.unstack(level = 'Medal')\n",
    "\n",
    "# Plot the DataFrame usa_medals_by_year\n",
    "usa_medals_by_year.plot()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
